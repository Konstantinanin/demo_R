---
title: "Περιγραφική και στατιστική ανάλυση συμφωνικών συμπλεγμάτων της Ελληνικής με βάση το χαρακτηριστικό [+/-λόγιο] στο γραπτό και προφορικό λόγο."
author: Κωνσταντίνα Αντωνοπούλου
date: 2/4/2021
output:
  html_document:
    theme: readable
    toc: true
    toc_depth: 3
    df_print: paged
bibliography: reference.bib  
    

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


### 1. Εισαγωγή

Στόχος της παρούσας εργασίας είναι να μελετήσει δύο σώματα κειμένων προφορικού και γραπτού λόγου και να αναδείξει τις διαφορές τους ως προς τη χρήση συγκεκριμένων συμφωνικών συμπλεγμάτων με βάση το βαθμό λογιότητάς τους. Αρχικά, δίνεται ένας ορισμός για τα Σώματα Κειμένων και παρουσιάζονται τα κύρια χαρακτηριστικά και οι εφαρμογές τους. Στη συνέχεια, δίνεται το θεωρητικό πλαίσιο του κυρίως θέματος της εργασίας, το οποίο ασχολείται με ζητήματα φωνολογίας. Το επόμενο κομμάτι αφορά την παρουσίαση, επεξεργασία και περιγραφική ανάλυση των γλωσσικών δεδομένων που αντλήθηκαν από τα σώματα κειμένων. Τέλος, διεξάγεται έλεγχος υπόθεσης με τη χρήση των εργαλείων στατιστικής ανάλυσης που παρέχει η γλώσσα προγραμματισμού R.



### 2. Τα Σώματα Κειμένων και η χρήση τους

Όπως ορίζει @tantos _«τα Σώματα Κειμένων (ΣΚ) αποτελούν σύνολο αυθεντικών γλωσσικών δεδομένων που συλλέγονται με συγκεκριμένα γλωσσολογικά και στατιστικά κριτήρια που αφορούν την αντιπροσωπευτικότητα της υπό εξέταση γλωσσικής ποικιλίας»_. Με άλλα λόγια, τα ΣΚ είναι κείμενα φυσικής γλώσσας και αξιοποιούνται για την διατύπωση και εξέταση γλωσσολογικών θεωριών και υποθέσεων εφόσον θεωρούνται αντιπροσωπευτικά της γλωσσικής ποικιλίας που εξετάζει ο ερευνητής. Η αντιπροσωπευτικότητα σχετίζεται άμεσα με τη δειγματοληψία, δηλαδή με την επιλογή των κειμένων που απαρτίζουν το ΣΚ. Έτσι, η συλλογή των δειγμάτων μας (των κειμένων) πρέπει να γίνεται με τέτοιο τρόπο, ώστε το ΣΚ να μην μεροληπτεί υπερεκπροσωπώντας ή υποεκπροσωπώντας τα φαινόμενα/στοιχεία που μελετώνται [@tantos].

Η χρήση των ΣΚ δεν περιορίζεται μόνο στη γλωσσολογική έρευνα, αλλά επεκτείνεται και σε εφαρμογές Γλωσσικής Τεχνολογίας, καθώς και στην έρευνα στις (ψηφιακές) ανθρωπιστικές και κοινωνικές επιστήμες [@tantos]. Επίσης, τα ΣΚ διαθέτουν ορισμένα χαρακτηριστικά, τα οποία συνιστούν μια πιο αξιόπιστη μεθοδολογία έρευνας, καθώς δεν βασίζονται στις διαισθήσεις των ομιλητών, αλλά σε προϋπάρχοντα γλωσσικά δεδομένα, ενώ παράλληλα η γλώσσα είναι αποτέλεσμα αυθόρμητης και όχι τεχνητής παραγωγής, όπως για παράδειγμα οι παραγωγές των ομιλητών στα πλαίσια ενός πειράματος [@goutsos]. Ταυτόχρονα, τα ΣΚ παρέχουν μια πιο αντικειμενική εκπροσώπηση της γλωσσικής χρήσης, σε αντίθεση με τις παραγωγές φυσικών ομιλητών, οι οποίες φέρουν ιδιοσυγκρασιακά και άρα πιο υποκειμενικά χαρακτηριστικά [@goutsos] [@tantos]. Τέλος, τα ΣΚ αποτελούνται από μεγάλο όγκο δεδομένων και είναι προσβάσιμα και διαχειρίσιμα εφόσον διατίθενται σε κάποια ηλεκτρονική βάση δεδομένων [@goutsos].

Στην παρούσα εργασία τα ΣΚ χρησιμοποιούνται για γλωσσολογική έρευνα, δηλαδή για τον έλεγχο μιας υπόθεσης και την αξιολόγηση των αποτελεσμάτων που θα προκύψουν. Πιο συγκεκριμένα, αξιοποιούνται δύο σώματα κειμένων προφορικού και γραπτού λόγου με σκοπό την ανίχνευση τάσεων στη χρήση συγκεκριμένων συμφωνικών συμπλεγμάτων. Τα ΣΚ αντλήθηκαν από την Εθνική Υποδομή Γλωσσικών Πόρων και Τεχνολογιών Clarin, η οποία είναι μια ηλεκτρονική βάση γλωσσικών δεδομένων και εργαλείων γλωσσικής τεχνολογίας. Το προφορικό ΣΚ αποτελείται από μεταγραφές συνεντεύξεων με 335.610 συνολικό αριθμό λέξεων. Το γραπτό ΣΚ αποτελείται από
διάφορα κείμενα κυρίως ειδησεογραφικού λόγου με ποικίλη θεματολογία συνολικού αριθμού 489.000 λέξεων. Στη συνέχεια, πραγματοποιήθηκε επισημείωση των λεξικών τεμαχίων. Όπως ορίστηκε από τον Leech (2005), επισημείωση είναι η «προσθήκη ερμηνευτικών γλωσσικών πληροφοριών σε ένα σώμα κειμένων» [@goutsos]. Η γλώσσα XML (eXtensible Markup Language) θεωρείται η πιο διαδεδομένη γλώσσα επισημείωσης. Στην εργασία αυτή, χρησιμοποιήθηκε το UAM Corpus tool, το οποίο είναι μια πλατφόρμα επισημείωσης με γραφικό περιβάλλον (GUI), το οποίο καθιστά πιο εύκολη τη χρήση από το γλωσσολόγο, ενώ παράλληλα διαθέτει και προκατασκευασμένα πλαίσια επισημείωσης. Για αυτήν την εργασία η επισημείωση έγινε χειροκίνητα χωρίς τη χρήση κάποιου εξειδικευμένου πλαισίου. Το UAM δημιούργησε αρχεία XML, τα οποία εισήχθησαν στην R και έπειτα έγινε εξαγωγή των ετικετοποιημένων λέξεων μέσω της γλώσσας Xpath. Στη συνέχεια, πραγματοποιήθηκαν διάφορες διαδικασίες με την R, έτσι ώστε τα δεδομένα μας να μετατραπούν σε μια μορφή που θα καθιστά έυκολη τη διαχείρισή τους για την πραγματοποίηση οπτικοποιήσεων και στατιστικών ελέγχων (data wrangling).



### 3. Θεωρητικό πλαίσιο

Η διάκριση λόγιου και μη λόγιου/λαϊκού χρηστικού επιπέδου απορρέει από τη διαχρονία καθώς και από το γλωσσικό ζήτημα [@book]. Όπως επισημαίνουν οι @book _«το λόγιο επίπεδο προήλθε από τη φυσική διαχρονική κληρονόμηση μέσω της γλώσσας της διοίκησης,της εκκλησιαστικής γλώσσας, του τυπικού προφορικού και γραπτού λόγου και του επιστημονικού λόγου, καθώς επίσης και από την προτυποποίηση της Αρχαίας Ελληνικής που οδηγεί στην επανεισαγωγή στοιχείων κυρίως στους διεθνισμούς της ορολογίας, και στην τεχνητή αναγέννηση στοιχείων ως απόρροια του γλωσσικού ζητήματος κυρίως με τη μορφή απολιθωμάτων της καθαρεύουσας»._ Η γλωσσική αυτή διχοτόμηση μετουσιώθηκε συγχρονικά σε μια χρηστική ποικιλία στη Νέα Ελληνική, η οποία συνδέεται με την ιδεολογία, τη γλωσσική πολιτική, την πραγματολογία και την κοινωνιογλωσσολογία» [@book].
 
Οι @book επιχειρούν να δημιουργήσουν ένα θεωρητικό πλαίσιο επαναπροσδιορίζοντας το λόγιο επίπεδο με βάση το χρηστικό κριτήριο έναντι του ετυμολογικού, κι αυτό γιατί υπάρχουν γλωσσικά στοιχεία που ναι μεν είναι κληρονομημένα με βάση την ετυμολογία τους από τα Αρχαία Ελληνικά, αλλά συγχρονικά δεν έχουν λόγια/τυπική χρήση. Το ετυμολογικό κριτήριο ωστόσο είναι και αυτό αναγκαίο στο βαθμό που αποτελεί στοιχείο περιφερειακότητας και απόκλισης μαρκάροντας ένα λεξικό τεμάχιο ως αρχαιοπρεπές και λόγιο. Με βάση τα παραπάνω, οι @book καθορίζουν το χαρακτηριστικό [+/- λόγιο ] ως υπώνυμο του τυπικού/μη τυπικού χρηστικού επιπέδου και το τοποθετούν σε ένα συνεχές, το οποίο καλύπτει τις ζώνες από το [+ λόγιο] στη νόρμα και από τη νόρμα στο [-λόγιο], ενώ στη μέση (στη νόρμα) υπάρχει επικαλυπτόμενη κατανομή των γλωσσικών στοιχείων, τα οποία εναλλάσσονται στη χρήση. Η χρηστική ευλυγισία που παρουσιάζουν τα γλωσσικά εκείνα στοιχεία που βρίσκονται κοντά στη νόρμα αποδίδονται -μεταξύ άλλων-  σε κοινωνιογλωσσολογικούς παράγοντες [@book].
 
 
                    + λόγιο 				    νόρμα 					- λόγιο
                      __________________________________________________________
                      
_1. Συνεχές λογιότητας._

Το [+/- λόγιο ] χαρακτηριστικό απαντάται σε όλα τα επίπεδα της γλωσσολογίας (φωνολογία, σύνταξη, σημασιολογία, πραγματολογία), καθώς και στο λεξιλόγιο. Οι @article επιχειρούν να ταξινομήσουν τα λόγια στοιχεία για κάθε επίπεδο της γλωσσολογίας. Στο επίπεδο της φωνολογίας δίνονται ενδεικτικά τα παρακάτω συμφωνικά συμπλέγματα:  /kt/, /pt/, /fθ/, /sθ/, /xθ/, /kv/, /nθ/ (_πτωχοκομείο, κτήμα, εδέχθη, έφθασε_). Αυτά ορίζονται ως ποικιλία, καθώς υπάρχουν και οι «ανταγωνιστές» τους, δηλαδή συμφωνικά συμπλέγματα που παρουσιάζουν ελεύθερη εναλλαγή στη χρήση με τα παραπάνω, όπως το /xt/, /ft/ κλπ. Στο πλαίσιο επαναπροσδιορισμού της έννοιας της λογιότητας με βάση το χρηστικό επίπεδο, θεωρείται αναγκαία η εξέταση όλων αυτών των τύπων στα Σώματα Κειμένων [@book]. Τα ΣΚ, όπως αναφέρθηκε και παραπάνω, μελετούν τη γλώσσα στη χρήση και αποτελούν πηγές ανατροφοδότησης της γλωσσολογικής θεωρίας.

Μια άλλη έρευνα στην οποία αξιοποιούνται Σώματα Κειμένων είναι η διατριβή της @mitsiaki, στην οποία επιχειρείται η κατασκεύη μιας διαβαθμιζόμενης (gradient) γραμματικής του ορθού σχηματισμού (wellformedness) για τα συμφωνικά συμπλέγματα /kt, xt, fθ, ft, pt, xθ/ που βρίσκονται σε θέση λεξικής έμβασης. Η διαβαθμιζόμενη γραμματική θα αποτυπωθεί σε ένα συνεχές από τα περισσότερο στα λιγότερο αποδεκτά συμφωνικά συμπλέγματα λαμβάνοντας υπόψιν, τόσο τους φωνοτακτικούς κανόνες της γραμματικής (markedness), όσο και την απόλυτη συχνότητά τους στα λήμματα του λεξικού (type frequency), αλλά και την συχνότητα χρήσης τους (token frequency) στα Σώματα Κειμένων. Όπως αναφέρει η @mitsiaki, η εύρωστη αντιπρoσώπευση κάποιων συμπλεγμάτων τόσο στο λεξικό, όσο και στα Σώματα Κειμένων συνιστά περισσότερο καλοσχηματισμένα συμπλέγματα.

Με βάση τα γραμματικά, φωνολογικά και φωνητικά κριτήρια το /fθ/ θεωρείται λιγότερο καλοσχηματισμένο και εύρωστο αντιληπτικά, καθιστώντας δυσκολότερη την εκμάθησή του τόσο στη Γ1 όσο και στη Γ2 [@mitsiaki]. Ταυτόχρονα, παραβιάζει τον κανόνα OCP[MANNER], ο οποίος ορίζει ως περισσότερο μαρκαρισμένα τα συμπλέγματα που είναι ίδια ως προς τον τρόπο άρθρωσης. Πράγματι, το /fθ/ αποτελείται από δύο τριβόμενα σύμφωνα στη σειρά και υπόκειται πολύ συχνά στη φωνολογική διαδικασία της ανομοίωσης (dissimilation) δίνοντας το /ft/. Ωστόσο, φαίνεται πως η λεξικοκεντρική προσέγγγιση συγκλίνει με τη γραμματική, προκρίνοντας το /ft/ έναντι του /fθ/. Δηλαδή, το /ft/ εκπροσωπείται σε μεγαλύτερο βαθμό στο λεξικό και στα ΣΚ συγκριτικά με το /fθ/. 

Στην περίπτωση της παρούσας εργασίας, μελετάμε τα /fθ/- /ft/ σε δύο ΣΚ προφορικού και γραπτού λόγου, είτε αυτά βρίσκονται σε θέση λεξικής έμβασης (φτάνω-φθάνω), είτε αποτελούν συνοπτικό παθητικό θέμα για παράδειγμα του μεσοπαθητικού αορίστου, της υποτακτικής ή του μέλλοντα (σκεφτώ- σκεφθώ, σκέφθηκα- σκέφτηκα). Το /fθ/ είναι το λόγιο, καθώς προήλθε από τα Αρχαία Ελληνικά και πιθανώς να χρησιμοποιείται περισσότερο σε γραπτά κείμενα, τα οποία χαρακτηρίζονται από μεγαλύτερη επισημότητα. Αντιθέτως, το /ft/ είναι ο ανομοιωμένος εναλλακτικός τύπος, που πιθανώς να χαρακτηρίζεται από εντονότερη προφορικότητα και άρα, να χρησιμοποιείται περισσότερο στον προφορικό απ'ότι στον γραπτό λόγο. Με βάση την παραπάνω σκέψη διατυπώνουμε τη μηδενική μας υπόθεση (H0).

### 4. Εισαγωγή και επεξεργασία δεδομένων  

Αρχικά θέτουμε το περιβάλλον εργασίας μας, καθώς εκεί βρίσκονται όλοι οι φάκελοι XML που θα εισάγουμε.

```{r}
setwd("C:/Users/ntina/Desktop")
```

Στη συνέχεια εισάγουμε τη βιβλιοθήκη για την εισαγωγή και επεξεργασία των XML και εισάγουμε το προφορικό Σώμα Κειμένων.

```{r}
library(xml2)
speech_Corpus <- read_xml("C:/Users/ntina/Desktop/Cclusters/finalspeechcorpus-consonant_cl.xml")
```

Έπειτα, χρησιμοποιώντας την εντολή xml_find_all εξάγουμε όλα τα επισημειωμένα στοιχεία και στη συνέχεια, με την επόμενη εντολή εξάγουμε μόνο τις λέξεις χωρίς τις ετικέτες τους.

```{r}
speech_tags <- xml_find_all(speech_Corpus, "//segment")
words <- trimws(xml_text(speech_tags))
 
```

Έπειτα εισάγουμε το tidyverse, το οποίο είναι μια συλλογή πακέτων κατάλληλων για επεξεργασία δεδομένων. To tidyverse μας επιτρέπει να χρησιμοποιήσουμε το σύμβολο  «%>%», το οποίο «ενώνει» τις συναρτήσεις (functions), παίρνοντας το αποτέλεσμα της μίας και εισάγοντάς το στην επόμενη. Έτσι, πάλι με την εντολή xml_find_all εξάγουμε όλα τα επισημειωμένα τεμάχια με τις ετικέτες τους και αυτή τη φορά, περνάμε το εξαγόμενο στη δεύτερη εντολή, η οποία εξάγει μόνο το όνομα τις ετικέτας που είναι ίδιο για όλα τα γλωσσικά τεμάχια (consonant_cl, δηλαδή συμφωνικό σύμπλεγμα), καθώς και την τιμή που έχει το κάθε ένα από αυτά (learned/not_learned δηλαδή λόγιο/μη λόγιο). Έπειτα, δημιουργούμε ένα dataframe.


```{r warning=FALSE, message= FALSE}
library(tidyverse)
speech_features <- xml_find_all(speech_Corpus, "//segment") %>% xml_attr("features")
speech_df <- data.frame(speech_features, words)

```


```{r}
# inspect speech_df 50 first elements
head(speech_df, n = 50)
```


Στη συνέχεια, εφαρμόζουμε κάποιες εντολές προκειμένου να ξεκαθαρίσουμε την πληροφορία που υπάρχει σε κάθε στήλη και σειρά του speech_df. Μετονομάζουμε στήλες και γραμμές και μετατρέπουμε όλα τα γράμματα σε μικρά, έτσι ώστε να μην θεωρούνται μοναδικές οι λέξεις που είναι ίδιες μεταξύ τους. Ταυτόχρονα, μετατρέπουμε τη στήλη consonant_clusters σε ένα factor με δύο επίπεδα.

```{r}
# rename column of features
names(speech_df)[1] <- "consonant_clusters"
``` 

```{r}
# rename attributes
speech_df$consonant_clusters[speech_df$consonant_clusters == "consonant_cl;not_learned"] <- "not_learned"
speech_df$consonant_clusters[speech_df$consonant_clusters == "consonant_cl;learned"] <- "learned"
```

```{r}
# convert features column to a factor with two levels
speech_df$consonant_clusters <- as.factor(speech_df$consonant_clusters)
print(levels(speech_df$consonant_clusters))
```

```{r}
# correct a mistake
speech_df$words[255] <- "επισκεφθούμε"
```

```{r}
# turn all characters to lowercase
speech_df$words=tolower(speech_df$words)
```


```{r}
head(speech_df, n = 50)
```

Το tidyverse που εισαγάγαμε παραπάνω περιέχει το πακέτο ggplot2, το οποίο είναι κατάλληλο για οπτικοποιήσεις. Δημιουργούμε ένα ggplot και παρατηρούμε τις κατανομές του λογίου/μη λογίου στο προφορικό κείμενο. Είναι εμφανές ότι υπάρχει διαφορά στην κατανομή των συμπλεγμάτων με πιο συχνόχρηστο το /ft/.

```{r}
# create a ggplot to show the distribution of the two types of consonant clusters
ggplot(speech_df) +
  aes(x = consonant_clusters, fill = consonant_clusters) +
  geom_bar() +
  ggtitle("Speech corpus") + theme_minimal()
```

Στη συνέχεια, εισάγουμε το γραπτό Σώμα Κειμένων, το οποίο αποτελείται από περισσότερα από ένα κείμενα. Έτσι, πρεπει να εισάγουμε διαφορετικά XML αρχεία στην R. Αρχικά, αποθηκεύουμε τα μονοπάτια των XML σε μια μεταβλητή.Έπειτα, δημιουργούμε μια δική μας συνάρτηση, η οποία θα επαναλαμβάνει ό,τι κάναμε και παραπάνω για το προφορικό κείμενο. Θα διαβάζει το XML, θα εξάγει ό,τι είναι κείμενο/λέξεις και τις ετικέτες/χαρακτηριστικά. Τέλος, θα δημιουργεί ένα dataframe με δύο στήλες.

```{r}
# assign to a variable a vector of the xml paths that we are going to import and process for the written corpus
xml_paths <- c("C:/Users/ntina/Desktop/Cclusters/Populismus_Data_01-consonant_cl.xml", "C:/Users/ntina/Desktop/Cclusters/written_corpus(economic)-consonant_cl.xml", 
               "C:/Users/ntina/Desktop/Cclusters/written_tanea-consonant_cl.xml", "C:/Users/ntina/Desktop/Cclusters/written_history-consonant_cl.xml")
```


```{r}
# create a function that opens and processes multiple xml files
process_xml <- function(xml_paths) {
  rdxml <- read_xml(xml_paths)
  extr_seg <- xml_find_all(rdxml, "//segment") 
  vls <- trimws(xml_text(extr_seg))
  extr_feats <- xml_find_all(rdxml, "//segment") %>% xml_attr("features")
               df<- data.frame(features= extr_feats, words = vls)
}
```

Εφαρμόζουμε την εντολή lapply, η οποία παίρνει ως πρώτο όρισμα τo διάνυσμα (vector) με τα μονοπάτια των XML αρχείων που δημιουργήσαμε παραπάνω, και για κάθε ένα από αυτά εφαρμόζει τη συνάρτηση που φτιάξαμε, ενώ στο τέλος επιστρέφει μία λίστα ίδιου μήκους με το κάθε στοιχείο στο οποίο εφαρμόστηκε η συνάρτηση. Στην συνέχεια, χρησιμοποιούμε την do.call, η οποία παίρνει ως πρώτο όρισμα μία εντολή -την rbind στην περίπτωσή μας- , ενώ ως δεύτερo όρισμα μπαίνει η λίστα που μας έδωσε η προηγούμενη εντολή. Η rbind εφαρμόζεται στη λίστα αυτή, και αυτό που κάνει είναι να ενώνει το κάθε τι στη λίστα σε σειρές ενός dataframe.

```{r}
# apply this function to each element of the vector we have created for the xml files of the written corpus
dflist <- lapply(xml_paths, function(x){process_xml(x)})
```

```{r}
# the lapply returns a list of the results of the function and the do.call combines them all into one df
written_df <- do.call(rbind, dflist)
```

Στη συνέχεια, ακολουθούμε περίπου τα ίδια βήματα όπως και για το προφορικό ΣΚ εφαρμόζοντας εντολές που θα μετονομάσουν στήλες, γραμμές και θα μετατρέψουν από κεφαλαία σε μικρά, έτσι ώστε να υπάρχει ομοιομορφία στο written_df. 

```{r}
# rename column
names(written_df)[1] <- "consonant_clusters"
```


```{r}
# change names
written_df$consonant_clusters[written_df$consonant_clusters == "consonant_cl;not_learned"] <- "not_learned"
written_df$consonant_clusters[written_df$consonant_clusters == "consonant_cl;learned"] <- "learned"
```

```{r}
# correct the name of a value
written_df$consonant_clusters[40] <-"learned"
```


```{r}
#turn all characters to lowercase
written_df$words=tolower(written_df$words)

head(written_df, 20)
```

Κάνουμε ένα ggplot για να ελέγξουμε την κατανομή των δεδομένων μας και στο γραπτό ΣΚ. Το γράφημα για το γραπτό ΣΚ δείχνει ότι το συγκεκριμένο ζεύγος συμπλεγμάτων χρησιμοποιείται σχεδόν εναλλακτικά με μια μικρή διαφορά στη χρήση του /fθ/, το οποίο παρουσιάζει λίγες περισσότερες εμφανίσεις.

```{r}
ggplot(written_df) +
  aes(x = consonant_clusters, fill = consonant_clusters) +
  geom_bar() +
  ggtitle("Written corpus")
```

Το επόμενο βήμα είνα να δημιουργήσουμε ένα dataframe με τις συχνότητες εμφάνισης της κάθε λέξης στο προφορικό ΣΚ. Εμφανίζοντας τις πρώτες 20 εγγραφές στο dataframe, δε λαμβάνουμε ιδιαίτερα χρήσιμες πληροφορίες γι'αυτό και το επόμενο βήμα είναι να δούμε τις λέξεις που εμφανίζονται πάνω από 9 φορές στο ΣΚ κάνοντας ένα απλό plot της baseR.

```{r}
# create a dataframe of the frequency of each word
freq_speech_df <- as.data.frame(table(speech_df$words))
head(freq_speech_df, n = 20)
names(freq_speech_df)[1] <- "words"
#plot words whose occurences exceed 9 times in the speech corpus
plot(droplevels(freq_speech_df[freq_speech_df$Freq > 9, ]), main = "Most frequent words in Speech Corpus") 
```



Το επόμενο γράφημα δείχνει τα ποσοστά εμφάνισης των πιο συχνόχρηστων λέξεων του προφορικού ΣΚ. Κατ'αρχάς παρατηρείται επανάληψη συγκεκριμένων ρημάτων με μόνες διαφορές τον χρόνο, την όψη ή το πρόσωπο. Το ρήμα «σκεφτώ» εμφανίζεται στο 15 % των περιπτώσεων. Μπορεί να είναι είτε τύπος της υποτακτικής ή του μέλλοντα. Στη συνέχεια, με περίπου 13 % ποσοστό εμφάνισης ακολουθεί το «φτάνει» που μπορεί να είναι απρόσωπο ή τριτοπρόσωπο στον ενεστώτα.

```{r}
# df with most frequent words in speech corpus
mostfreq_sdf <- freq_speech_df %>% filter(Freq >9)
frq_plot_speech <-ggplot(mostfreq_sdf, aes(x = words, y = (Freq/nrow(speech_df))*100)) + ylim(0,20) +geom_bar(stat= "identity", fill = c("#9f79ee", "#ab82ff", "#9370db", "#c71585", "#7a378b","#7b68ee","#9370db",  "#d15fee", "#ba55d3")) + ggtitle("Most frequent words in Speech Corpus in %")
frq_plot_speech
```

Κάνουμε το ίδιο και για το γραπτό ΣΚ, δηλαδή δημιουργούμε dataframe με τις συχνότητες εμφάνισης της κάθε λέξης, και έπειτα κάνουμε ένα plot με τις πιο συχνόχρηστες λέξεις, η εμφάνιση των οποίων ξεπερνά τις 9 φορές. Μετά κάνουμε ένα ggplot με τα ποσοστά εμφάνισης των πιο συχνόχρηστων λέξεων. Παρατηρούμε, ότι τα ρήματα «έφτασε» και «επισκέφθηκε» χρησιμοποιούνται γύρω στο 13 %. Είναι φανερό ότι στο γραπτό ΣΚ χρησιμοποιείται συχνά η τελευταία λέξη που περιέχει το λόγιο συμφωνικό σύμπλεγμα, γεγονός που μας δίνει κάποια επιπλέον πληροφορία.

```{r}
# create a df of the frequency of words in written corpus
freq_written_df <- as.data.frame(table(written_df$words))
names(freq_written_df)[1] <- "words"

#plot words whose occurences exceed 10 times in written corpus
plot(droplevels(freq_written_df[freq_written_df$Freq > 9, ]), main = "Most frequent words in Written Corpus") 

mostfreq_wdf <- freq_written_df %>% filter(Freq >9)
frq_plot_written <-ggplot(mostfreq_wdf, aes(x = words, y = (Freq/nrow(written_df))*100)) + ylim(0,20)+ geom_bar(stat = "identity", fill= c("#8d5524","#ffdbac","#c68642","#e0ac69", "#ffdbac", "#e0ac69", "#c68642", "#e0ac69", "#ffdbac", "#e0ac69")) +ggtitle("Most frequent words in Written Corpus in %")
frq_plot_written
```

Στη συνέχεια, χρησιμοποιώντας την εντολή mutate του tidyverse, δημιουργούμε μια επιπλέον στήλη με εγγραφές «speech» και «written» για το speech_df και το written_df αντίστοιχα και ονομάζουμε τη στήλη «corpus». Έπειτα, ενώνουμε τα δύο dataframes και φτιάχνουμε ένα ραβδόγραμμα, με δύο ράδβδους η καθεμία από τις οποίες δείχνει τις κατανομές των συμπλεγμάτων στο written και στο speech που βρίσκονται στον άξονα του x. Το όρισμα position = "stack" μας επιτρέπει να δούμε τις κατανομές του λογίου/μη λογίου σε μία ράβδο για το κάθε ένα ΣΚ. Έτσι, κάνουμε πιο ξεκάθαρες συγκρίσεις παρατηρώντας τις διαφορές των δύο ΣΚ σε ένα γράφημα. Οι διαφορές τους είναι εμφανείς. Στο προφορικό ΣΚ υπάρχουν ελάχιστες εμφανίσεις της λόγιας  παραλλαγής, ενώ στο γραπτό ΣΚ το λόγιο σύμπλεγμα εμφανίζεται σε παραπάνω από τις μισές λέξεις που υπάρχουν.
 
```{r}
# create a new column named corpus for both dfs
speech_df <- speech_df %>% mutate(corpus = "speech")
written_df <- written_df %>% mutate(corpus = "written")

# bind the dfs
whole_df <- rbind(speech_df, written_df)
whole_df$corpus <- as.factor(whole_df$corpus)

# barplot showing occurences of learned and not learned cclusters in the two corpora
ggplot(whole_df, aes(fill = consonant_clusters,x = corpus)) +
  geom_bar(position = "stack") +
  theme_minimal()
```

Σε αυτό το σημείο εκτελούμε ακόμη ένα βήμα που αποτελεί επίσης περιγραφικό κομμάτι των δεδομένων μας και αφορά τον έλεγχο των τεμαχίων που βρίσκονται πριν και μετά από το /fθ/ και /ft/. Προτού, ξεκινήσουμε να γράφουμε τον κώδικα, καταλαβαίνουμε ότι σίγουρα οτιδήποτε προηγείται ή ακολουθεί τα συμφωνικά συμπλέγματα θα είναι φυσικά φωνήεν. Οπότε φτιάχνουμε ένα vector με όλα τα φωνήεντα και τις διαφορετικές ορθογραφίες τους. Στη συνέχεια, δημιουργούμε μια συνάρτηση, η οποία  παίρνει ως πρώτο όρισμα ένα dataframe και ώς δεύτερο όρισμα τον αριθμό των σειρών του dataframe. Η συνάρτηση αυτή δημιουργεί ένα άδειο dataframe με τέσσερις στήλες: η πρώτη στήλη είναι η στήλη «consonant_clusters» του dataframe που έχουμε περάσει ως όρισμα, η οποία καταγράφει σε κάθε σειρά αν το σύμπλεγμα είναι λόγιο/μη λόγιο. Οι άλλες τρεις στήλες θα περιέχουν τις λέξεις και τα φωνήεντα που βρίσκονται πριν και μετά από το σύμπλεγμα αντίστοιχα.

Έπειτα, αναθέτουμε σε μια μεταβλητή cnt το 1, η οποία στην ουσία θα αποτελέσει έναν μετρητή. Στη συνέχεια, φτιάχνουμε μια for loop, η οποία θα πάρει την πρώτη λέξη από τη στήλη «words», όπου βρίσκονται οι λέξεις του dataframe που έχουμε εισάγει ως όρισμα. Η if ελέγχει αν αυτή η λέξη έχει μέσα της το /ft/ και αν ναι, περνάει στην πρώτη θέση της στήλης «Words» του νέου dataframe που δημιουργήσαμε. Στη συνέχεια με strsplit χωρίζουμε αυτή τη λέξη στο σημείο που υπάρχει το /ft/ και το αποθηκεύουμε σε μια μεταβλητή w. Μετά, πάλι με strsplit χωρίζουμε ό,τι βρίσκεται στην πρώτη θέση του w, δηλαδή πριν από το /ft/ ουσιαστικά, στην περίπτωση που υπάρχει, και το περνάμε στην μεταβλητή w1. Μετά με paste ενώνουμε το προτελευταίο με το τελευταίο τεμάχιο πριν από το /ft/ και το αναθέτουμε στη μεταβλητή w1. Αυτό το κάνουμε για να διασφαλίσουμε ότι θα περιλάβουμε τα δίφθογγα (ει, ου, αι κλπ.). Μετά, με μία for loop περνάει ένα j από όλα τα φωνήεντα της λίστας vowels. Καθώς εκτελείται η επανάληψη οι φωλιασμένες if ελέγχουν αν το w1 δεν είναι κενό, δηλαδή αν υπάρχει κάτι πριν το /ft/ και αν δεν είναι κενό, τότε ελέγχεται αν το w1 υπάρχει μέσα στο j. Όταν το w1 θα είναι μέσα στο j, θα αποθηκευτεί στη θέση 1 της στήλης «Before» του νέου μας dataframe. Στη συνέχεια διακόπτουμε με break τη δομή επανάληψης, έτσι ώστε το j να σταματήσει την επανάληψη στη λίστα vowels. Αν το w1 ωστόσο είναι άδειο, τότε δεν περνάει στην επόμενη if, αλλά πάει στην else, όπου αποθηκεύεται πάλι στη θέση 1 της στήλης «Before» η λέξη «Onset». Σ'αυτή την περίπτωση δηλαδή, δεν υπάρχει τίποτα πριν το /ft/ και άρα, το /ft/ βρίσκεται σε θέση λεξικής έμβασης. Στη περίπτωση που το /ft/ δεν υπάρχει στο i, στην πρώτη πρώτη μας if, τότε θα υπάρχει /fθ/ στο i και σ'αυτήν την περίπτωση θα περάσει στην παρακάτω else στην οποία εκτελούνται οι ίδιες εντολές με τη μόνη διαφορά ότι τώρα έχουμε /fθ/.

Στο τέλος, το cnt αυξάνεται κατά 1 και ξεκινάει να τρέχει ο κώδικας πάλι από την αρχή μέχρι να φτάσει τον αριθμό που έχουμε βάλουμε ως δεύτερο όρισμα στη συνάρτηση και που αποτελεί τον αριθμό των σειρών του dataframe μας. Στο δεύτερο σκέλος της συνάρτησης, πραγματοποιείται πάλι η ίδια σειρά εντολών με τη μόνη διαφορά ότι ελέγχουμε ό,τι βρίσκεται στη δεύτερη θέση του w, δηλαδή μετά το /ft/ ή /fθ/. Τέλος, εφαρμόζουμε τη συνάρτηση περνώντας ως ορίσματα την πρώτη φορά το speech_df Και τον αριθμό των σειρών του και τη δεύτερη το written_df με τον αντίστοιχο αριθμό σειρών. Παρατηρούμε τις πρώτες 50 εγγραφές των δύο dataframe και στη συνέχεια δημιουργούμε διαγράμματα για να συνοψίσουμε την πληροφορία.

```{r}

vowels <- c("εί","ει","οι","οί","ου","ού","αι","αί","ε",'έ','α',"ά","η","ή","ω",
            "ώ","ι","ί","υ","ύ","ο","ό")

# we create an empty data frame with 4 columns

# function that iterates through every word of the "words" column of the df and extracts the vowels that come before and after the consonant clusters (/ft/ or /fθ/) and places them in the cols of the df we created

createdf <- function(df,no){
  all <- data.frame(df$consonant_clusters,
                    Words=character(no),
                    Before=character(no),
                    After=character(no))
# iterate through words of the df we put as argument. If /ft/ is in i, then i is placed in position 1 of the col "Words" of the df we created
  cnt <- 1
  for(i in df$words){
    if(grepl("φτ",i,fixed=TRUE)){
      all$Words[cnt] <- i
# split the word in with "ft" and extract what's on the the first position
      w <- strsplit(i,"φτ")[[1]]
      w1 <- strsplit(w[1],"")[[1]]
# extract the penultimate and ultimate segments in the case that we have a diphong
      w1 <- paste(w1[length(w1)-1],w1[length(w1)],sep = "")
# iterate through vowels and check if w1 is empty, if it is, it goes down in else and in the col "Before" the word "Onset" is written, because /ft/ is an onset. If it's not empty we check if j (any of the vowels that j iterates through) is in w1 and we assign it to position 1 of the "Before" col of our new df. We interrupt the iteration.
      for(j in vowels){
        if(!(is_empty(w1))){
          if(grepl(j,w1,fixed=TRUE)){
            all$Before[cnt] <- j
            break
          } 
        }
        else{
          all$Before[cnt] <- "Onset"
        }
      }
    }
# if /ft/ is not in i, /fθ/ is in i, so it goes down in this else. The same commands are executed. but with /fθ/ this time.
    else{
      all$Words[cnt] <- i
      w <- strsplit(i,"φθ")[[1]]
      w1 <- strsplit(w[1],"")[[1]]
      w1 <- paste(w1[length(w1)-1],w1[length(w1)],sep = "")
      for(j in vowels){
        if(!(is_empty(w1))){
          if(grepl(j,w1,fixed=TRUE)){
            all$Before[cnt] <- j
            break
          } 
        }
        else{
          all$Before[cnt] <- "Onset"
        }
      }
    }
    cnt <- cnt + 1
  }
# The second part is exactly like the first one with the only difference that after the word is split, we extract what's on the right part of the split word, that is after the consonant clusters by which the word has been split.
  cnt <- 1
  for(i in df$words){
    if(grepl("φτ",i,fixed=TRUE)){
      all$Words[cnt] <- i
      w <- strsplit(i,"φτ")[[1]]
      w2 <- strsplit(w[2],"")[[1]]
      w2 <- paste(w2[1],w2[2],sep = "")
      for(j in vowels){
        if(!(is_empty(w2))){
          if(grepl(j,w2,fixed=TRUE)){
            all$After[cnt] <- j
            break
          } 
        }
        else{
          all$After[cnt] <- "Onset"
        }
      }
    }
    else{
      all$Words[cnt] <- i
      w <- strsplit(i,"φθ")[[1]]
      w2 <- strsplit(w[2],"")[[1]]
      w2 <- paste(w2[1],w2[2],sep = "")
      for(j in vowels){
        if(!(is_empty(w2))){
          if(grepl(j,w2,fixed=TRUE)){
            all$After[cnt] <- j
            break
          } 
        }
        else{
          all$After[cnt] <- "Onset"
        }
      }
    }
    cnt <- cnt + 1
  }
  all
}

# apply the function in the two dfs for the written and speech corpus
speech_split <- createdf(speech_df,296)
head(speech_split, n = 50)
written_split <- createdf(written_df,300)
head(written_split, n = 50)
```

Παρακάτω, δημιουργούμε barplots για να συγκρίνουμε τις διαφορές στις εμφανίσεις των φωνηέντων πριν από τα συμφωνικά συμπλέγματα στα δύο ΣΚ.

```{r fig.width= 8}
# it helps us to compare two plots by having them side by side
par(mfrow= c(1,2))

# we create a barplot of the table for cols "df.consonant_clusters" (learned/not_learned) and the vowels that come before cclusters of the speech corpus which shows the occurences of the learned/not_learned consonant clusters (/ft/ - /fθ/) based on what comes before.
s <- as.factor(as.character(speech_split$Before))
p1 <- barplot(table(speech_split$df.consonant_clusters,speech_split$Before),main="Vowels Before cclusters in Speech",ylim = c(0,200),col = c("#576afb","#ae48e2"),legend=TRUE)

# show occurences of each vowel
text(p1, y = table(s), label = table(s), pos = 3, cex = 0.8, col = "red")

w <- as.factor(as.character(written_split$Before))
p2 <- barplot(table(written_split$df.consonant_clusters, written_split$Before),main="Vowels Before cclusters in Written",ylim=c(0,180),col = c("#18afe2","#FF4000","#25a5e2","#254586","#aefcde","#a25f4e"), legend = TRUE) 

# show occurences of each vowel
text(p2, y = table(w), label = table(w), pos = 3, cex = 0.8, col = "dark green")
```


Έπειτα, δημιουργούμε barplots για να ελέγξουμε τα φωνήεντα που ακολουθούν τους δύο τύπους συμφωνικών συμπλεγμάτων στα δύο ΣΚ. 

```{r fig.width= 8}
#have two plots side by side
par(mfrow=c(1,2))

# turn characters into factors  
s2 <- as.factor(as.character(speech_split$After))
# create barplot for what comes after the consonant clusters for the speech corpus
p3 <-barplot(table(speech_split$df.consonant_clusters, speech_split$After),main="Vowels After cclusters in Speech",ylim = c(0,150), col = c("yellow", "dark red", "coral", "red", "yellow", "coral"), legend= TRUE)
# add number of vowel occurences in the plot
text(p3, y = table(s2), label = table(s), pos = 3, cex= 0.8, col = "red")

#turn characters into factors
w2 <- as.factor(as.character(written_split$After))
#barplot for what comes after cclusters for the written corpus
p4 <- barplot(table(written_split$df.consonant_clusters,written_split$After),main="Vowels After cclusters in Written",ylim = c(0,150), col = c("dark blue", "blue", "light green", "blue", "light green", "light green", "green", "light green"), legend= TRUE)
# add number of vowel occurence in the plot
text(p4,y= table(w2), label = table(w2), pos = 3, cex= 0.8, col = "dark green")
```

### 5. Έλεγχος υπόθεσης

Η περιγραφή των δεδομένων μάς έχει δώσει μια εικόνα όσον αφορά τις κατανομές των συμφωνικών συμπλεγμάτων στα δύο ΣΚ. Είναι φανερό ότι στο προφορικό ΣΚ οι εμφανίσεις των λόγιων συμπλεγμάτων είναι πολύ λιγότερες συγκριτικά με το γραπτό ΣΚ. Ταυτόχρονα, παρατηρείται εξίσου αξιοσημείωτη διαφορά στην κατανομή του μη λόγιου συμπλέγματος ανάμεσα στα δύο ΣΚ. Παρακάτω, ο πίνακας συχνοτήτων μας δείχνει τις κατανομές με ακρίβεια.

```{r}
# contingency table
t<- table(whole_df$corpus, whole_df$consonant_clusters)
t
#barplot
barplot(t, beside = T, legend= T, col = c("yellow", "blue"))
```

Ωστόσο, μπορούμε να ελέγξουμε αν και πόσο στατιστικά σημαντική είναι η απόκλιση αυτή στις κατανομές των συμφωνικών συμπλεγμάτων στα δύο ΣΚ. Σε αυτή την περίπτωση θα πραγματοποιήσουμε το τεστ ανεξαρτησίας χ2 (chi-square test of independence), το οποίο θα βασιστεί στην υπόθεση ότι οι κατηγορικές μεταβλητές μας είναι ανεξάρτητες, δηλαδή, τα είδη των ΣΚ δεν επηρεάζουν τις κατανομές των λόγιων/μη λόγιων συμφωνικών συμπλεγματων σε αυτά.

H0 : Δεν υπάρχει διαφορά στις κατανομές των συμφωνικών συμπλεγμάτων στα προφορικό και γραπτό ΣΚ. Οι μεταβλητές είναι ανεξάρτητες.


Η1 : Υπάρχει στατιστικά σημαντική διαφορά στις κατανομές στα δύο ΣΚ. Οι μεταβλητές παρουσιάζουν κάποια συσχέτιση.

Το τεστ ανεξαρτησίας χ2 υπολογίζεται με βάση τις παρατηρούμενες και τις αναμενόμενες τιμές των μεταβλητών με τον παρακάτω τύπο:

$x2 = sum((O-E)^2/E)$

όπου,

O = Observed values (παρατηρούμενες τιμές)

Ε = Expected values (αναμενόμενες τιμές υπό τη μηδενική υπόθεση)

Ο πίνακας συνάφειας (contingency table) που είδαμε παραπάνω δίνει τις συχνότητες, δηλαδή της παρατηρούμενες τιμές για το κάθε κελί. 
Για να βρούμε τις αναμενόμενες τιμές για κάθε κελί πολλαπλασιάζουμε το άθροισμα των τιμών της σειράς με το άθροισμα των τιμών της στήλης και διαιρούμε με το σύνολο όλων των παρατηρήσεων. Για παράδειγμα για το πρώτο κελί στον πίνακα συνάφειας θα κάναμε: $(32+264)*(32+152)/592$, το οποίο μας δίνει ως αναμενόμενη τιμή 92 εμφανίσεις του λόγιου συμπλέγματος στο προφορικό κόρπους. Βεβαίως η R, μας δίνει τη δυνατότητα να τα ελέγξουμε αυτά πολύ εύκολα. Παρακάτω διεξάγουμε το τεστ ανεξαρτησίας χ2. Για να διαπιστώσουμε αν υπάρχει συσχέτιση των μεταβλητών μας πρέπει να ελέγξουμε αν υπάρχει στατιστική σημαντικότητα, δηλαδή να δούμε ποια είναι η πιθανότητα να έχουμε τις παρατηρούμενες τιμές δεδομένης της μηδενικής υπόθεσης. Αυτό το ορίζουμε με βάση την τιμή a, η οποία συνήθως είναι 0.05%. Αν δηλαδή το p-value είναι < 0.05%, τότε θα αναγκαστούμε να απορρίψουμε τη μηδενική μας υπόθεση. Αυτό σημαίνει ότι η πιθανότητα να έχουμε τις παρατηρούμενες τιμές (ή και πιο ακραίες) υπό τη μηδενική υπόθεση είναι πολύ μικρή για να θεωρηθεί τυχαία και άρα, υπάρχει στατιστική σημαντικότητα, δηλαδή οι μεταβλητές μας πιθανότατα έχουν κάποια συσχέτιση. Από την άλλη, αν το p-value > 0.05%, τότε «αποτυγχάνουμε» να απορρίψουμε τη μηδενική υπόθεση.

```{r}
# perform chi square test
ch_test<-chisq.test(table(whole_df$corpus, whole_df$consonant_clusters))
ch_test

# chck the expected values
ch_test$expected
```
Κάνοντας το τεστ, διαπιστώνουμε ότι το p-value < 0.05%. Επομένως, απορρίπτουμε τη μηδενική υπόθεση, η οποία λέει ότι δεν υπάρχει κάποια συσχέτιση μεταξύ των ΣΚ και της εμφάνισης των λόγιων/μη λόγιων συμπλεγμάτων. Το αποτέλεσμα μας δείχνει ότι υπάρχει στατιστική σημαντικότητα και πιθανώς να υπάρχει κάποια συσχέτιση μεταξύ των ΣΚ (προφορικό/γραπτό) και των συμφωνικών συμπλεγμάτων (λόγιο/μη λόγιο). Παρ'όλα αυτά, δεν μας δείχνει την ισχύ και την κατεύθυνση της συσχέτισης.

Στη περίπτωσή μας, θα μπορούσαμε να κάνουμε και το τεστ ακρίβειας του Fisher, το οποίο ενδείκνυται για μικρότερα δείγματα.Παρατηρούμε ότι μας δίνει ένα p-value πολύ κοντά σε αυτό που έδωσε το τεστ χ2.

```{r}
# we also perform fisher's test which is more suitable for smaller samples
fisher.test(t)
```

 Με την παρακάτω εντολή, ελέγχουμε τα υπόλοιπα (Pearson's residuals), τα οποία μας δείχνουν κατά πόσο διαφέρουν οι παρατηρούμενες από τις αναμενόμενες τιμές. Οι θετικές τιμές δείχνουν κατά πόσο οι παρατηρούμενες τιμές υπερέχουν των αναμενόμενων, ενώ οι αρνητικές δείχνόυν το πόσο χαμηλότερες τιμές από το αναμενόμενο παρατηρήθηκαν.
 
```{r}
#Pearson's residuals
ch_test$residuals
# association plot showing the positive and negative residuals (red is negative - black is positive)
assocplot(t) 
```
 
Αυτές οι τιμές μας δίνουν κάποια στοιχεία για το είδος συσχέτισης των μεταβλητών μας. Για παράδειγμα, οι παρατηρούμενες συχνότητες του λόγιου συμπλέγματος στο προφορικό ΣΚ ειναι -6.2111939 από τις αναμενόμενες υπό τη μηδενική υπόθεση, κάτι που μας δείχνει ότι υπάρχει αρνητική συσχέτιση του προφορικού ΣΚ και της εμφάνισης λόγιων συμπλεγμάτων σε αυτά. Από την άλλη, παρατηρούμε θετική συσχέτιση του γραπτού ΣΚ με την εμφάνιση λόγιων συμπλεγμάτων, όπως φαίνεται από το γεγονός ότι οι παρατηρούμενες τιμές διαφέρουν κατά 6.170387 από τις αναμενόμενες τιμές.

### 6. Συμπεράσματα

Με βάση τα παραπάνω αντιλαμβανόμαστε ότι η κατανομή του λόγιου συμπλέγματος στα ΣΚ είναι που μας οδηγεί σε κάποιες παρατηρήσεις όσον αφορά τη χρήση του. Στο γραπτό ΣΚ χρησιμοποιείται παραπάνω από το αναμενόμενο, γεγονός που μας οδηγεί στη σκέψη, ότι τα λόγια συμπλέγματα χρησιμοποιούνται σε κείμενα που ενδεχομένως χαρακτηρίζονται από μεγαλύτερη επισημότητα. Από την άλλη, δεν ισχύει αυτό για το προφορικό ΣΚ, στο οποίο ενδεχομένως μεταβλητές όπως ρυθμός ομιλίας αλλά και πιο ανεπίσημος λόγος να καθιστούν λίγοτερο εύχρηστο ή κατάλληλο το λόγιο σύμπλεγμα. Ώστόσο, το μη λόγιο σύμπλεγμα φαίνεται να χρησιμοποιείται εκτενώς και στα δύο είδη Σωμάτων Κειμένων, γεγονός που συγχρονικά, με βάση το συνεχές λογιότητας, το καθιστά πιο κοντά στη νόρμα.

Η παρούσα εργασία θα μπορούσε να αποτελέσει την αρχή για πιο συστηματική μελέτη διαφόρων εναλλακτικών τύπων συμφωνικών συμπλεγμάτων σε πλήθος κειμένων με διαφορετικό βαθμό επισημότητας, έτσι ώστε να δοθεί μια πιο ολοκληρωμένη εικόνα της χρήσης τους. Ταυτόχρονα, κρίνεται αναγκαία η δημιουργία προφορικών Σωμάτων Κειμένων, τα οποία να παρέχονται με τις αντίστοιχες ηχρογραφήσεις, έτσι ώστε να είναι όσο πιο ακριβής η μέτρηση των στοιχείων που μας αφορούν στην έρευνα, ειδικά από τη στιγμή που έχουμε να κάνουμε με φωνολογικά/φωνητικά δεδομένα.

### Βιβλιογραφία








